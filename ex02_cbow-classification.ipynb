{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b673220-1259-4ab1-8b29-5efd7fe85ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "from functools import cached_property\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db207190-377c-4a95-b3f7-5788b59dac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/pytorch/text/blob/v0.18.0/torchtext/data/utils.py#L17-L21\n",
    "RAW_PATTERN_TO_REPLACEMENT_DICT = {\n",
    "    r\"\\'\": \" '  \", \n",
    "    r\"\\\"\": \"\", \n",
    "    r\"\\.\": \" . \", \n",
    "    r\"<br \\/>\": \" \", \n",
    "    r\",\": \" , \", \n",
    "    r\"\\(\": \" ( \", \n",
    "    r\"\\)\": \" ) \", \n",
    "    r\"\\!\": \" ! \", \n",
    "    r\"\\?\": \" ? \", \n",
    "    r\"\\;\": \" \", \n",
    "    r\"\\:\": \" \", \n",
    "    r\"\\s+\": \" \",\n",
    "}\n",
    "\n",
    "PATTERN_TO_REPLACEMENT_DICT = {\n",
    "    re.compile(pattern): replacement\n",
    "    for pattern, replacement \n",
    "    in RAW_PATTERN_TO_REPLACEMENT_DICT.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd3e430e-196b-4764-93bc-61c0a9ed641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(\n",
    "    line: str, \n",
    "    pattern_to_replacement_dict: dict[re.Pattern, str] = PATTERN_TO_REPLACEMENT_DICT\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    line = line.lower()\n",
    "    for pattern, replacement in pattern_to_replacement_dict.items():\n",
    "        line = pattern.sub(replacement, line)\n",
    "    return line\n",
    "\n",
    "def tokenize(line: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return line.split()\n",
    "\n",
    "def encode(\n",
    "    token_list: list[str],\n",
    "    token_to_index: dict[str, int],\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        token_list:\n",
    "        token_to_index:\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    return list(map(token_to_index.__getitem__, token_list))\n",
    "\n",
    "def preprocess(\n",
    "    data: str, \n",
    "    token_to_index: dict[str, int]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (str): IMDB review\n",
    "        token_to_index: a dict that maps tokens to nonnegative ints\n",
    "    \"\"\"\n",
    "    data = normalize(data)\n",
    "    data = tokenize(data)\n",
    "    data = encode(data, token_to_index=token_to_index)\n",
    "    data = torch.tensor(data, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e655ad1-69df-45ed-a6a6-e4a5cc33052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c1c86c-a906-49ec-91a7-b7f8776501e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc4b60e-7923-4a14-9397-c2c3e285a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data_dir.exists():\n",
    "    download_and_extract_archive(\n",
    "        url=url, \n",
    "        download_root=data_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69e078c-7108-41b4-af21-560982040160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_counter(data_dir) -> Counter:\n",
    "    token_counter = Counter()\n",
    "    for split in ['train', 'test']:\n",
    "        for label in ['neg', 'pos']:\n",
    "            label_dir = data_dir / 'aclImdb' / split / label\n",
    "            path_list = list(label_dir.glob('*.txt'))\n",
    "            for path in tqdm.tqdm(path_list, desc=f'{split=}, {label=}, {len(path_list)=}'):\n",
    "                with open(path) as stream:\n",
    "                    data = stream.read()\n",
    "                token_counter += Counter(tokenize(normalize((data))))\n",
    "    return token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ffda825-3a32-42f8-aca8-01ae6b8eeb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split='train', label='neg', len(path_list)=12500: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:23<00:00, 534.18it/s]\n",
      "split='train', label='pos', len(path_list)=12500: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:47<00:00, 262.90it/s]\n",
      "split='test', label='neg', len(path_list)=12500: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [01:24<00:00, 148.22it/s]\n",
      "split='test', label='pos', len(path_list)=12500: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [02:06<00:00, 99.08it/s]\n"
     ]
    }
   ],
   "source": [
    "token_counter = get_token_counter(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdf59a1e-242f-44da-b84b-9355fe4ab35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = [token for token, _ in token_counter.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db6f4556-580c-41c3-ab7f-dcb6bc4b80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: index of 0\n",
    "token_to_index = {token: index for index, token in enumerate(token_list, start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e847c006-2f87-4ae0-951d-c9dc2faca8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "\n",
    "    LABEL_NAME_LIST = ['neg', 'pos']\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path, \n",
    "        token_to_index: dict[str, int],\n",
    "        train: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        split = 'train' if train else 'test'\n",
    "        split_dir = data_dir / 'aclImdb' / split\n",
    "            \n",
    "        self.example_list = []\n",
    "        for label, label_name in enumerate(self.LABEL_NAME_LIST):\n",
    "            label_dir = split_dir / label_name\n",
    "            path_list = list(label_dir.glob('*.txt'))\n",
    "            for each in tqdm.tqdm(path_list):\n",
    "                with open(each) as stream:\n",
    "                    text = stream.read()\n",
    "                    data = preprocess(text, token_to_index=token_to_index)\n",
    "                    self.example_list.append((text, data, label))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.example_list)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.example_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0071e6e-d5cb-476d-911e-d149f3ce2399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88df2c0d-0c68-43de-9b14-d57aa2aa6384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:02<00:00, 4487.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:02<00:00, 4359.19it/s]\n"
     ]
    }
   ],
   "source": [
    "train_set = IMDBDataset(data_dir=data_dir, token_to_index=token_to_index, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01951de7-39c7-4a7e-a71d-e4afaeab0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = random_split(\n",
    "    dataset=train_set,\n",
    "    lengths=(0.8, 0.2),\n",
    "    generator=torch.Generator().manual_seed(1337)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ee13508-c6a8-4f13-9eee-b91a77cca7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3bca27-7a09-42d5-a5e1-d6f131e1642a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 2512, 1: 2488})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(val_set[index][2] for index in range(len(val_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47b44e39-c25d-4bfd-ad7a-753c57035e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    text, data, label = zip(*batch)\n",
    "    text = list(text)\n",
    "    length = torch.tensor([len(each) for each in data], dtype=torch.long)\n",
    "    data = pad_sequence(\n",
    "        sequences=data, \n",
    "        batch_first=True, \n",
    "        padding_value=0,\n",
    "    )\n",
    "    label = torch.tensor(label)\n",
    "    return text, data, length, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b00ffa6b-9ade-4ea1-8e7a-9242970a7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_set,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dea6b8b7-9e96-463e-bb89-1d2889abac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBoW(nn.Module):\n",
    "\n",
    "    def __init__(self, num_tokens: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=(num_tokens + 1),\n",
    "            embedding_dim=32,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor,\n",
    "        length: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: a tensor\n",
    "        Returns:\n",
    "            output:\n",
    "        Shape:\n",
    "            input: (N, L)\n",
    "            output: (N, )\n",
    "        \"\"\"\n",
    "        h = self.embedding(x)\n",
    "        h = h.sum(dim=1) / length.unsqueeze(dim=-1).to(h.dtype)\n",
    "        logits = self.mlp(h)\n",
    "        logits = logits.squeeze()\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65ddca7e-37c3-4704-b2ca-d3b1a424e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBoW(\n",
    "    num_tokens=len(token_to_index),\n",
    ")\n",
    "optimizer = SGD(\n",
    "    params=model.parameters(),\n",
    "    lr=1, # \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b0933-5792-45f9-8ddf-94024b6a313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "threshold = 0.5\n",
    "\n",
    "for epoch in range(max_epochs + 1):\n",
    "    if epoch > 0:\n",
    "        model.train()\n",
    "        for _, x, x_len, y_true in tqdm.tqdm(train_loader, desc='training'):\n",
    "            optimizer.zero_grad()\n",
    "            y_logits = model(x=x, length=x_len)\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                input=y_logits, \n",
    "                target=y_true.float()\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # validation\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "\n",
    "        for _, x, x_len, y_true in tqdm.tqdm(val_loader, desc='validation'):\n",
    "            y_logits = model(x=x, length=x_len)\n",
    "            y_score = y_logits.sigmoid()\n",
    "            y_pred = y_score.gt(threshold).long()\n",
    "\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                input=y_logits, \n",
    "                target=y_true.float(), \n",
    "                reduction='sum',\n",
    "            )\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_total += len(y_true)\n",
    "            val_correct += y_pred.eq(y_true).sum().item()\n",
    "    \n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        print(f'{epoch=: >6d}: Loss={val_loss:.3f} Accuracy={100 * val_acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43007122-8ec2-4f3e-b110-6744478b9333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lec-24-2-dl",
   "language": "python",
   "name": "lec-24-2-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
